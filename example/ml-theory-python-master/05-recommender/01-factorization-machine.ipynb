{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Factorization Machine\n",
    "----\n",
    "\n",
    "### Concept\n",
    "- Factorization Machine is general predictor like SVM. FM calculate all of pair-wise interaction between variables. So, FM can overcome the situation `cold-start` because of pair-wise vector factorization. It works like latent vector, but break the independence of the interaction parameter by factorizating them. Therefore, this algorithm can have a similar effect to using SVD++ with multiple variables.\n",
    "- This Algorithm works well especially in recommender system like e-commerce. Because of the sparsity in implicit data and content meta-information.\n",
    "- [Paper is here](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)\n",
    "- [My implemented code is here](https://github.com/yoonkt200/ml-theory-python/tree/master/nn-recommender/FM.py)\n",
    "\n",
    "### Model Equation\n",
    "- Conceptional model equation\n",
    "\n",
    "$$ \\hat{y}(x) = w_0 + \\sum_{i=1}^{n}w_i x_i + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} <v_i, v_j> x_i x_j $$\n",
    "\n",
    "$$ <v_i, v_j> = \\hat{w}_{ij} $$\n",
    "\n",
    "- FM have a closed model equation that can be computed in linear time O(kn), but actually O(kmd) because of zero-values.  \n",
    "\n",
    "$$ 0.5\\sum_{i=1}^{n}\\sum_{j=1}^{n} <v_i, v_j> x_i x_j - 0.5\\sum_{i=1}^{n}<v_i, v_i> x_i x_i $$\n",
    "\n",
    "$$ 0.5(\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\sum_{f=1}^{k} v_{i,f} v_{j,f} x_i x_j - \\sum_{i=1}^{n}\\sum_{f=1}^{k} v_{i,f} v_{i,f} x_i x_i) $$\n",
    "\n",
    "$$ 0.5\\sum_{f=1}^{k}((\\sum_{i=1}^{n} v_i x_i)(\\sum_{j=1}^{n} v_j x_j) - \\sum_{i=1}^{n} {v_i}^2{x_i}^2) $$\n",
    "\n",
    "$$ 0.5\\sum_{f=1}^{k}((\\sum_{i=1}^{n} v_i x_i)^2 - \\sum_{i=1}^{n} {v_i}^2{x_i}^2) $$\n",
    "\n",
    "- Below code is implementation of FM's equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pre-trained parameters\n",
    "b = 0.3\n",
    "w = np.array([0.001, 0.02, 0.009, -0.001])\n",
    "v = np.array([[0.00516, 0.0212581, 0.150338, 0.22903],\n",
    "              [0.241989, 0.0474224, 0.128744, 0.0995021], \n",
    "              [0.0657265, 0.1858, 0.0223, 0.140097], \n",
    "              [0.145557, 0.202392, 0.14798, 0.127928]])\n",
    "\n",
    "# Equation of FM model\n",
    "def inference(data):\n",
    "    num_data = len(data)\n",
    "    scores = np.zeros(num_data)\n",
    "    for n in range(num_data):\n",
    "        feat_idx = data[n][0]\n",
    "        val = np.array(data[n][1])\n",
    "        \n",
    "        # linear feature score\n",
    "        linear_feature_score = np.sum(w[feat_idx] * val)\n",
    "        \n",
    "        # factorized feature score\n",
    "        vx = v[feat_idx] * (val.reshape(-1, 1))\n",
    "        cross_sum = np.sum(vx, axis=0)\n",
    "        square_sum = np.sum(vx*vx, axis=0)\n",
    "        cross_feature_score = 0.5 * np.sum(np.square(cross_sum) - square_sum)\n",
    "        \n",
    "        # Model's equation\n",
    "        scores[n] = b + linear_feature_score + cross_feature_score\n",
    "\n",
    "    # Sigmoid transformation for binary classification\n",
    "    scores = 1.0 / (1.0 + np.exp(-scores))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Inference test for 3 case\n",
    "data = [[[0, 1, 3], # feature index \n",
    "         [0.33, 1, 1]], # feature value\n",
    "        [[2],\n",
    "         [1]],\n",
    "        [[0, 1, 2, 3],\n",
    "         [0.96, 1, 1, 1]]]\n",
    "\n",
    "inference(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Learning FM\n",
    "- The equation computed in linear time. So we can use SGD with below gradients. And in most cases add L2 Regularization on training model.\n",
    "\n",
    "$$ 1, \\hspace{1cm} if \\hspace{0.3cm} \\theta \\hspace{0.2cm} is \\hspace{0.2cm} w_0 $$\n",
    "\n",
    "$$ x_i, \\hspace{1cm} if \\hspace{0.3cm} \\theta \\hspace{0.2cm} is \\hspace{0.2cm} w_i $$\n",
    " \n",
    "$$ x_i \\sum_{j=1}^{n} v_{j,f}x_{j} - v_{i,f} {x_i}^2 \\hspace{1cm} if \\hspace{0.3cm} \\theta \\hspace{0.2cm} is \\hspace{0.2cm} v_{i,f} $$  \n",
    "\n",
    "### Binary Classification by FM\n",
    "- The FM classification model follow this rules.\n",
    "    - 1. h(x) = add sigmoid function from original h(x)\n",
    "    - 2. cost function is based on binary-cross entropy(or MLE)\n",
    "    - 3. parameter has different 3 type : bias(b), linear weight(w), latent weight(v)\n",
    "    - 4. gradient is based on binary-cross entropy and add FM parameter's gradient (Above `Learning FM chapter`)\n",
    "\n",
    "$$ h(x) = \\hat{y}(x) = sigmoid(0.5\\sum_{f=1}^{k}((\\sum_{i=1}^{n} v_i x_i)^2 - \\sum_{i=1}^{n} {v_i}^2{x_i}^2)) $$\n",
    "\n",
    "$$ Cost(\\Theta) = -1/n()\\sum_{i=1}^{n}[y^{(i)}log(h(x^{i})) + (1-y^{(i)})log(1-h(x^{(i)}))] $$\n",
    "\n",
    "$$ SGD repeat \\{ $$\n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha (h(x^{i})-y^{i}) * Gradient $$\n",
    "\n",
    "$$ \\} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
