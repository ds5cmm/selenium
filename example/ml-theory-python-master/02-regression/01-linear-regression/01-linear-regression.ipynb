{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Regression Analysis\n",
    "\n",
    "Regression analysis is finding relationship between Dependent variable and Independent variable. There two ways to find the result. Deterministic model and Probabilistic model. \n",
    "\n",
    "----\n",
    "\n",
    "### Deterministic Model\n",
    "- Deterministic model is finding f(x) that outputs the value $ \\hat{y} $ most similar to to dependent variable $ y $.\n",
    "- Linear regression analysis is as follows\n",
    "\n",
    "$$ \\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n = w_0 + w^Tx $$\n",
    "\n",
    "- $ w_0 $, $ ... $, $ w_n $ are calls `coefficient` or `paramters`.\n",
    "\n",
    "----\n",
    "\n",
    "### OLS(Ordinary Least Square)\n",
    "- OLS is one of Deterministic model and using RSS(Residual Sum of Squares).\n",
    "- residual vector $ e $ and RSS are as belows\n",
    "\n",
    "$$ e = y-\\hat{y} = y-WX $$\n",
    "\n",
    "$$ RSS = \\sum_{j=1}^{n}(y_i-\\hat{y_i})^2 = \\sum_{j=1}^{n}(y_i-w_ix_i)^2 $$\n",
    " \n",
    "$$ L = RSS = e^Te $$\n",
    "\n",
    "$$ = (y-WX)^T(y-WX) $$\n",
    "\n",
    "$$ = y^Ty - 2y^TWX + (WX)^TWX $$\n",
    "\n",
    "- To find $ W $, the partial derivative to find the minimum value of the gradient of Loss is as follows.\n",
    "\n",
    "$$ \\frac{dL}{dw} = -2X^Ty + 2X^TXw$$\n",
    "\n",
    "- And the optimization condition is belows. \n",
    "\n",
    "$$ \\frac{dL}{dw} = 0 $$\n",
    "\n",
    "$$ X^Ty = X^TXw$$\n",
    "\n",
    "- So, **`if` $ X^TX $ `has Inverse matrix`**, the conclusion is as follow.\n",
    "\n",
    "$$ w^* = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "----\n",
    "\n",
    "### MLE(Maximum likelihood estimator)\n",
    "\n",
    "Maximum likelihood estimation (MLE) is a technique used for estimating the parameters of a given distribution, using some observed data.\n",
    "\n",
    "- Expression as $ L(parameters | data ) $.\n",
    "- It's little bit different from probability : $ P(data | parameters) $\n",
    "- For example, if a population is known to follow a “normal distribution” but the “mean” and “variance” are unknown, MLE can be used to estimate them using a limited sample of the population. MLE does that by finding particular values for the parameters (mean and variance) so that the resultant model with those parameters (mean and variance) would have generated the data.\n",
    "- PDF of Gaussian distribution is : $ (\\frac{1}{\\sqrt{2\\pi\\sigma^2}} * e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}) $\n",
    "- Another example\n",
    "    - we have 3 data points : 2, 2.5, 3\n",
    "    - let's suppose that the values are from normal(gaussian) distribution\n",
    "    - we have some data from a model\n",
    "    - but we don't know about parameters\n",
    "    - in this situation, we can use MLE\n",
    "----\n",
    "### Calculating Likelihood\n",
    "\n",
    "Likelihood is also calculated from PDF functions but by calculating the joint probabilities of data points from a particular PDF function\n",
    "\n",
    "$$ L(parametes | data) = \\prod_{i=1}^{m} f(data_i | parameters) $$\n",
    "\n",
    "- Now, we assume that $ \\mu=2, \\sigma^2=1 $.\n",
    "- So, Likelihood when model $ N(\\mu=2, \\sigma^2=1): $\n",
    "\n",
    "$$ L(\\mu=2, \\sigma^2=1 | x = 2, 2.5, 3) = PDF(x=2) * PDF(x=2.5) * PDF(x=3) $$\n",
    "\n",
    "$$ L(2, 1 | x = 2, 2.5, 3) = f(x=2|2,1) * f(x=2.5|2,1) * f(x=3|2,1) $$\n",
    "\n",
    "$$ L(2, 1 | x = 2, 2.5, 3) = (\\frac{1}{\\sqrt{2\\pi1^2}} * e^{-\\frac{(2-2)^2}{2*1^2}}) * (\\frac{1}{\\sqrt{2\\pi1^2}} * e^{-\\frac{(2.5-2)^2}{2*1^2}}) * (\\frac{1}{\\sqrt{2\\pi1^2}} * e^{-\\frac{(3-2)^2}{2*1^2}}) $$\n",
    "\n",
    "$$ L(2, 1 | x = 2, 2.5, 3) = 0.39 * 0.35 * 0.24 = 0.03276$$ \n",
    "\n",
    "- How can we calculate the precise value of $ \\mu, \\sigma^2 $?\n",
    "\n",
    "$$ L(\\mu=2, \\sigma^2=1 | x = 2, 2.5, 3) = \\prod_{i=1}^{3} f(x_i | \\mu, \\sigma^2) $$\n",
    "\n",
    "$$ (\\frac{1}{\\sqrt{2\\pi\\sigma^2}} * e^{-\\frac{(2-\\mu)^2}{2\\sigma^2}}) * (\\frac{1}{\\sqrt{2\\pi\\sigma^2}} * e^{-\\frac{(2.5-\\mu)^2}{2\\sigma^2}}) * (\\frac{1}{\\sqrt{2\\pi\\sigma^2}} * e^{-\\frac{(3-\\mu)^2}{2\\sigma^2}})$$\n",
    "\n",
    "$$ = (\\frac{1}{\\sqrt{2\\pi\\sigma^2}})^3 * e^{-\\frac{(2-\\mu)^2}{2\\sigma^2}} * e^{-\\frac{(2.5-\\mu)^2}{2\\sigma^2}} * e^{-\\frac{(3-\\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "$$ ln(L(\\mu=2, \\sigma^2=1 | x = 2, 2.5, 3)) $$ \n",
    "\n",
    "$$ = -3ln(\\sqrt{2\\pi\\sigma^2}) - {\\frac{(2-\\mu)^2}{2\\sigma^2}} - {\\frac{(2.5-\\mu)^2}{2\\sigma^2}} - {\\frac{(3-\\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "$$ = -3ln(\\sqrt{2\\pi\\sigma^2}) - {\\frac{19.25 - 15\\mu + 3\\mu^2}{2\\sigma^2}} $$\n",
    "\n",
    "- Now we can find the ML estimator for $ \\mu $. Since this equation is a 2-th quadratic function, the maximum point is Inflection point of $\\mu$.\n",
    "\n",
    "$$ \\frac{\\delta(ln(L(\\mu, \\sigma^2 | x)))}{\\delta\\mu} = \\frac{6\\mu - 15}{2\\mu^2}$$\n",
    "\n",
    "$$ \\mu = 2.5 $$\n",
    "\n",
    "- Now we know MLE of $\\mu$. Like this, we can get MLE of $\\sigma^2$ by derivative partial differential of $\\sigma$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### MAP(Maximum A Posterior estimation)\n",
    "\n",
    "MLE is simplest parameter estimation method, but it is very sensitive by observed data points. So, we use MAP to overcome this situation.\n",
    "\n",
    "- MLE find parameter like below,\n",
    " \n",
    "$$ argmax f(data_i | parameters) $$\n",
    "\n",
    "- But, MAP is different. \n",
    "\n",
    "$$ argmax f(parameters | data_i) $$\n",
    "\n",
    "- Unfortunately, we don't know about $f(parameters | data_i)$.\n",
    "- At this point, We need `Bayes' Theorem.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Linear Regression with MLE\n",
    "\n",
    "In linear regression with MLE, we need a assuming that `y` has a normal distribution. And we assuming that $X$(Independent variable) is fixed & random. So, Dependent variable project to residuals, and residuals assuming that normal distribution. \n",
    "\n",
    "> “The independent variable (y values) is assumed be in a normal distribution”\n",
    "\n",
    "$$ \\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n $$\n",
    "\n",
    "- And our ML value of estimator is $\\hat{y}$. So from $f(x)$, we get a set of values as means.\n",
    "- The follow graph show this concept. That is each labels in the data point have their own mean and variance in a normal distribution.\n",
    "\n",
    "![MLE of regression](../img/MLE.png)\n",
    "\n",
    "- So, $\\hat{y}$ ~ $ N(WX,0)$\n",
    "- And we can derive like this.\n",
    "\n",
    "> - \"Residuals(=Error, $\\hat{y}-y$) are normally distributed\"\n",
    "> - Mean of residuals is 0\n",
    "> - Variance of residuals is $\\sigma^2$\n",
    "\n",
    "- So, residuals follow this.\n",
    "\n",
    "$$ \\epsilon \\approx N(0, \\sigma^2) $$\n",
    "\n",
    "- Now we have two ideas. One is $\\hat{y}$ ~ $ N(WX,0)$ and other is $ \\epsilon \\approx N(0, \\sigma^2) $. And there are some facts.\n",
    "\n",
    "> - E($y$) = E($\\hat{y} + \\epsilon$)\n",
    "> - E($y$) = E($\\hat{y}) + E(\\epsilon$)\n",
    "> - E($y$) = XW + 0 = XW\n",
    "> - Variance($y$) = Variance($\\hat{y} + \\epsilon$)\n",
    "> - Variance($y$) = Variance($\\hat{y}) + Variance(\\epsilon$)\n",
    "> - Variance($y$) = 0 + $\\sigma^2$ \n",
    "\n",
    "- So, the result is\n",
    "\n",
    "$$ y \\approx N(WX, \\sigma^2) $$\n",
    "\n",
    "- Now we can define the likelihood of regression.\n",
    "\n",
    "$$ N(WX, \\sigma^2 | x) = \\prod_{i=1}^{n} Gaussian(x_i) $$\n",
    "\n",
    "$$ = (\\frac{1}{\\sqrt{2\\pi\\sigma^2}})^n * e^{-\\frac{\\sum_{i=1}^{n}(y_i-x_iw)^2}{2\\sigma^2}} $$\n",
    "\n",
    "$$ = (\\frac{1}{\\sqrt{2\\pi\\sigma^2}})^n * e^{-\\frac{(Y-WX)^T(Y-WX)}{2\\sigma^2}} $$\n",
    "\n",
    "- Then we can get log-likelihood. And it is exactly same as OLS.\n",
    "\n",
    "![MLE of regression](../img/MLE_math.png)\n",
    "\n",
    "- If our data satisfies normal distribution, MLE of $W$ is exactly same with OLS either RMLE(Restricted ML) and ML. But standard error(in Gaussian, $\\sigma$) is little bit different.\n",
    "- Restricted ML is generate likelihood based on residuals. \n",
    "- MLE of standard error is square root of Hassian diagonal element. And the Hassian 2-depth derivative of LogLikelihood.\n",
    "- Standard error also estimated by above process like $W$.\n",
    "\n",
    "> Conclusion : ML estimation can be useful when the variance is different for each section of data, or when the distribution is different for each section of data. below images show these cases.\n",
    "\n",
    "![Variance_difference_example](../img/variance.png)\n",
    "![Chi_Square_MLE](../img/chi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "----\n",
    "### Gradient Descent\n",
    "The other way of estimate $W$ is optimization. And Gradient Descent is the basic of first-order iterative optimization.\n",
    "\n",
    "- Gradient Descent using partial derivatives of cost function.\n",
    "- Basic theorem is here. (alpha is learning rate)\n",
    "- It is important to make cost function convex.\n",
    "\n",
    "$$ \\Theta = \\Theta - \\alpha * \\frac{\\delta L}{\\delta \\Theta} $$\n",
    "\n",
    "----\n",
    "#### references\n",
    "- https://medium.com/quick-code/maximum-likelihood-estimation-for-regression-65f9c99f815d\n",
    "- https://en.wikipedia.org/wiki/Gradient_descent \n",
    "- https://blog.naver.com/leerider/100189193480\n",
    "- http://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/3-4-UnivarMLE.html\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
