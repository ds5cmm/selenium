{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Regularization\n",
    "----\n",
    "\n",
    "### Norm\n",
    "- Norm is a measure function of vector's size or distance.\n",
    "- There are several ways to calculate Norm. p-norm, maximum-norm...\n",
    "- The most representative is p-norm.\n",
    "- `L1 Norm` : If p=1, it calls Taxicab Norm or Manhattan Norm. \n",
    "- `L2 Norm` : If p=2, it calls Euclidean Norm.\n",
    "\n",
    "$$ ||x||_p := (\\sum_{i=1}^{n} |x_i|^p)^{1/p} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n",
      "7.416198487095663\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def p_norm(vector, p):\n",
    "    return math.pow(sum(np.power(vector, p)), 1/p)\n",
    "\n",
    "arr1 = np.array([1,2,3,4,5])\n",
    "print(p_norm(arr1, 1))\n",
    "print(p_norm(arr1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "- With reference to Norm, we can understand two type of loss. (`L1 loss`, `L2 loss`)\n",
    "\n",
    "$$ L1 = \\sum_{i=1}^{n}|y_i - f(x_i)| $$ \n",
    "\n",
    "$$ L2 = \\sum_{i=1}^{n}(y_i - f(x_i))^2 $$ \n",
    "\n",
    "- L1 loss calls `LAD`(Least Absolute Deviations).\n",
    "- L2 loss calls `LSE`(Least Square Error). This is the same as MSE(Mean Square Error).\n",
    "- Robustness : Robustness is how much loss is affected when we have an outlier. If we have many outliers, LSE more increasing than LAD. So, if we want to remove outlier's effect, choose L1 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7000000000000002\n",
      "0.1100000000000001\n"
     ]
    }
   ],
   "source": [
    "def l1_loss(y, y_pred):\n",
    "    return sum(np.abs(y-y_pred))\n",
    "\n",
    "def l2_loss(y, y_pred):\n",
    "    return sum(np.square(y-y_pred))\n",
    "\n",
    "arr1 = np.array([1,2,3,4,5])\n",
    "arr2 = np.array([1.1,2.2,2.9,4.1,4.8])\n",
    "print(l1_loss(arr1, arr2))\n",
    "print(l2_loss(arr1, arr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Regularization\n",
    "\n",
    "- Regularization is a penalty for model's complexity. As a result, it prevent overfitting. And it helps model generalization.\n",
    "- Generalization means balanace between model's feature. If one feature have super higher effectiveness than other, model's generalization is broken. \n",
    "- There are many ways regularization for prevent overfit and balancing the feature's effectiveness. (e.g. L1 regularization, L2 regularization, Dropout)\n",
    "- `Summary : Adding loss so that it doesn't overfit the training data so perfectly`\n",
    "- L1 Regularization (if regression use L1, it calls Rasso model)\n",
    "\n",
    "$$ Cost(W,b) = 1/m\\sum_{i=1}^{m}L(\\hat{y_i}, y_i) + \\lambda ||w||_1 $$\n",
    " \n",
    "- L2 Regularization (if regression use L2, it calls Ridge model)\n",
    "\n",
    "$$ Cost(W,b) = 1/m\\sum_{i=1}^{m}L(\\hat{y_i}, y_i) + \\lambda ||w||_2 $$\n",
    "\n",
    "- L1 has the effect of feature selection. So it works well on sparse dataset. On the other hand, L2 does feature generalization. The below chart explain these phenomenon.\n",
    "\n",
    "![regularization](img/regularization.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
