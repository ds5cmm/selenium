{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "----\n",
    "\n",
    "Entropy is a feature value that describes the shape of a probability distribution, and is also a value indicating the amount of information a probability distribution has.\n",
    "\n",
    "### Define of Entropy\n",
    "- There are 3 different probability distribution in binary set.\n",
    "    - Probability distribution $ Y_1 : P(Y=0) = 0.5, P(Y=1) = 0.5 $ \n",
    "    - Probability distribution $ Y_2 : P(Y=0) = 0.8, P(Y=1) = 0.2 $ \n",
    "    - Probability distribution $ Y_3 : P(Y=0) = 1.0, P(Y=1) = 0.0 $ \n",
    "- In terms of bayesian statistics, this distributions has some information like this.\n",
    "    - 1. Knowing nothing about Y\n",
    "    - 2. Believe that Y is 0, but it may not be.\n",
    "    - 3. 100% believe in Y is 0.\n",
    "- And the entropy reperesent this difference of information.\n",
    "- **In summary**, entropy is a `numerical expression of the certainty or amount of information in the probability distribution`.\n",
    "    - If the probability of specific value in probability distribution go higher and other value's probability go lower, then entropy is decrease. But the other value's probability go higher, entropy is increase.\n",
    "    - In other words, entropy is a `characteristic value indicating what the probability distribution looks like`.\n",
    "    - In physics, the degree to which a substance's state is dispersed is defined as entropy.\n",
    "- Mathematically, entropy is a function has input(pdf) and output(numerical result).\n",
    "    - For example, if random variable Y follow the discrete probability distribution with K'th class, entropy defined like below.\n",
    "    - $ H[Y] = -\\sum_{k=1}^{K}p(y_k)log_2p(y_k) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Gini impurity (ÏßÄÎãà Î∂àÏàúÎèÑ)\n",
    "\n",
    "Similar concept with entropy, gini is a measure of where the probability distribution is it. The difference with entropy is do not use log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Joint Entropy\n",
    "\n",
    "Joint entropy is the entropy using joint probability.\n",
    "\n",
    "$$ H[X, Y] = -\\sum_{i=1}^{K_x}\\sum_{j=1}^{K_y}p(x_i, y_j)log_2p(x_i, y_j) $$\n",
    "\n",
    "### Conditional Entropy\n",
    "\n",
    "Conditional entropy is a method to measure which random variable ùëã helps predict the value of another random variable ùëå.\n",
    "\n",
    "$$ H[Y | X=x_i] = -\\sum_{j=1}^{K_y}p(y_j|x_i)log_2p(y_j|x_i) $$\n",
    "\n",
    "$$ H[Y | X] = -\\sum_{i=1}^{K_x}\\sum_{j=1}^{K_y}p(x_i, y_j)log_2p(y_j|x_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Cross Entropy\n",
    "\n",
    "Cross entropy usually using at classification problem. \n",
    "\n",
    "- Cross entropy of two probability distribution x,y : $H[p,q]$\n",
    "    - If discrete probability distribution : $ H[p,q] = -\\sum_{k=1}^{K}p(y_k)log_2q(y_k) $\n",
    "    - If continuous probability distribution : $ H[p,q] = -\\int_y p(y)log_2q(y)dy $\n",
    "- Cross entropy's input is not a random variable. It is pdf.\n",
    "\n",
    "### Cross Entropy for Classification\n",
    "\n",
    "Now show one example - binary classification.\n",
    "\n",
    "- In binary classification, Y is 0 or 1. \n",
    "- p is Y's probability distribution.\n",
    "    - So, When Y=1, $ p(Y=0) = 0, p(Y=1) = 1 $\n",
    "    - And when Y=0, $ p(Y=1) = 1, p(Y=0) = 0 $\n",
    "- And q is X's probability distribution. If it is Bern distribution, then\n",
    "    - $ q(Y=0) = 1 - \\mu, q(Y=1) = \\mu $\n",
    "- Therefore, The cross entropy of p,q is\n",
    "    - When Y=1, $ H[p,q] = -p(Y=0)log_2q(Y=0) -p(Y=1)log_2q(Y=1) = -log_2\\mu $\n",
    "    - When Y=0, $ H[p,q] = -p(Y=0)log_2q(Y=0) -p(Y=1)log_2q(Y=1) = -log_2(1-\\mu) $\n",
    "- Now, we can use this for loss function of classification model. Because when Y=1, $\\mu$ go lower(predict go uncorrect), cross entropy bigger. And when Y=0, $\\mu$ go higher(predict go uncorrect), cross entropy bigger.\n",
    "\n",
    "----\n",
    "\n",
    "- If we have N data : log-loss : $ -\\frac{1}{N}\\sum_{i=1}^{N}(y_i log_2 \\mu_i + (1 - y_i) log_2 (1-\\mu_i)) $\n",
    "- Same method, we can calculate the multi-class classification.\n",
    "    - categorical log-loss : $ -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^{K}(g(y_i, k)*log_2p(y_i=k) $\n",
    "    - $ g(y_i, k) $ means : if $y_i$ == $k$, then 1 else 0. It is a indicator function.\n",
    "    - $ p(y_i=k) $ means : probability of $ y_i = k $ calculated by classification model(e.g. softmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Kullback-Leibler divergence\n",
    "\n",
    "KL divergence is the calculated value that how are different the shape of probability distribution between $p(y)$ and $q(y)$. Generally written as $KL(p||q)$\n",
    "\n",
    "- $KL(p||q)$ = $ H[p,q] - H[p] $ (It means cross entropy - p's entropy. So sometimes KLD calls `relative entropy`)\n",
    "- = $ \\sum_{i=1}^{K}p(y_i)log_2(\\frac{p(y_i)}{q(y_i)}) $ (In discrete probability distribution)\n",
    "- If two probability distribution is same, KLD is converge to 0.\n",
    "\n",
    "### Mutual Information\n",
    "\n",
    "Mutual information can replace the `Correlation coefficient`. It is derive from the assumptions about the independence between variables. If X, Y is independent each other, The combined probability density function is equal to the product of the peripheral probability density function. $ p(x,y) = p(x)p(y) $.\n",
    "\n",
    "- Mutual information is KL divergence between $p(x,y)$ and $p(x)*p(y)$. \n",
    "- This method measures the correlation between two random variables by measuring the difference between the combined probability density function and the peripheral probability density function. If two random variable is independent each other, mutual information is 0.\n",
    "\n",
    "$$ MI[X,Y] = \\sum_{i=1}^{K}p(x_i, y_i)log_2(\\frac{p(x_i, y_i)}{p(x_i)*p(y_i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### references\n",
    "- https://datascienceschool.net/view-notebook/d3ecf5cc7027441c8509c0cae7fea088/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
