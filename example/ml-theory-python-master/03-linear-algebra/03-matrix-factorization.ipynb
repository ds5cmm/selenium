{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "\n",
    "Matrix factorization is one of model-based algorithm (or Latent Factor Model). Using the nature of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define\n",
    "\n",
    "- Basic concept of matrix factorization is `matrix completion`.\n",
    "- Fill the below image's `?` is main purpose of matrix completion.\n",
    "\n",
    "<img src=\"img/mf.png\" alt=\"MF\" style=\"width: 300px;\"/>\n",
    "\n",
    "- Matrix's row, column is factor of concept (e.g. Row: user, Column: Movie, Value: Rating)\n",
    "- And matrix factorization divide it two parts(P, Q). And they called `Latent Factor`.\n",
    "    - $ R \\approx P X Q^T = \\hat{R} $\n",
    "    - And Latent factor has their own dimension `k`.\n",
    "    - $ \\hat{r_{ij}} = p_i^T q_j = \\sum_{k=1}^k p_{ik}q_{kj} $\n",
    "    - The example of using latent factor\n",
    "        - **e.g.** When we have (User, Movie, Rating) matrix, we implicitically knows about every user has prefered genre. In this situation, user latent factor's dimension(k) maybe works like `prefer genre`. if k=3, each dimension value represent preference score of each genre. (romance = -1, action = 2, comedy = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Cost function & Train\n",
    "\n",
    "- Cost function of Matrix Factorization is below.\n",
    "\n",
    "$$ Cost = \\sum_{i,u \\in R_{train}} (r(i,u) - \\hat{r}(i,u) )^2 + \\lambda(b(i)^2 + b(u)^2 + ||p(i)||^2 ||q(u)||^2) $$\n",
    "\n",
    "- And we can use Gradient Descent.\n",
    "\n",
    "$$ \\grave{p_{ik}} = p_{ik} +\\alpha \\frac{dCost}{dp}$$\n",
    "\n",
    "\n",
    "$$ \\grave{q_{kj}} = q_{kj} +\\alpha \\frac{dCost}{dq}$$\n",
    "\n",
    "- Here is my process of deviation\n",
    "\n",
    "<img src=\"img/process1.png\" alt=\"MF\" style=\"width: 300px;\"/>\n",
    "<img src=\"img/process2.png\" alt=\"MF\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 7X5 Matrix\n",
    "R = np.array([\n",
    "        [1, 0, 0, 1, 3],\n",
    "        [2, 0, 3, 1, 1],\n",
    "        [1, 2, 0, 5, 0],\n",
    "        [1, 0, 0, 4, 4],\n",
    "        [2, 1, 5, 4, 0],\n",
    "        [5, 1, 5, 4, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "    ])\n",
    "\n",
    "# P, Q is (7 X k), (k X 5) matrix\n",
    "P = np.random.normal(size=(R.shape[0], 4))\n",
    "Q = np.random.normal(size=(R.shape[1], 4))\n",
    "\n",
    "# b_P, b_Q is (7 X 1), (5 X 1)\n",
    "b_P = np.ones(R.shape[0])\n",
    "b_Q = np.ones(R.shape[1])\n",
    "b = np.mean(R[np.where(R != 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.14544264,  0.76617751, -0.15163809, -1.14918166],\n",
       "       [-1.19423342,  1.62504796,  0.14128756,  0.07143081],\n",
       "       [-1.48406439, -1.2017517 ,  0.5767384 ,  0.49059089],\n",
       "       [-0.76688018, -1.12450154,  1.19667241, -2.22018095],\n",
       "       [ 0.63085357,  0.32312185,  0.81577999,  1.01556337],\n",
       "       [ 0.87686163,  0.60873574,  0.7956862 , -0.82178764],\n",
       "       [-0.41426823,  0.78840721, -0.67253972, -0.68151125]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24450083, -0.56573807, -0.38928391,  0.49186013],\n",
       "       [-0.23816813, -0.73399454,  1.10309967,  0.75727339],\n",
       "       [ 0.68286032,  2.53800163,  0.0578524 ,  0.75766522],\n",
       "       [ 0.38358876, -0.82099596,  1.36663131,  0.09294516],\n",
       "       [ 0.35038026,  1.91152701,  0.09604263, -0.52097559]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.37118526,  2.71821442,  6.4381786 ,  4.08721513,  7.04095046],\n",
       "       [ 3.94368164,  3.89250802,  7.96208329,  2.99838392,  7.25515227],\n",
       "       [ 5.65042787,  6.83415649,  0.93252199,  5.84205877,  1.57354699],\n",
       "       [ 3.85672146,  5.23769848, -0.39967323,  6.64900861,  3.44428597],\n",
       "       [ 4.43580764,  5.87243597,  6.65842961,  5.77687787,  4.97886979],\n",
       "       [ 3.3181783 ,  4.19066317,  6.15804784,  5.43852227,  6.56630999],\n",
       "       [ 4.17276672,  2.85291931,  5.75373514,  2.80226431,  6.24327761]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_hat = b + b_P[:, np.newaxis] + b_Q[np.newaxis:, ] + P.dot(Q.T)\n",
    "R_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Alternating Least Squares (with Implicit feedback)\n",
    "----\n",
    "\n",
    "ALS is specific optimizer for Matrix Factorization.\n",
    "\n",
    "- Learn `User Latent Factor` and `Item Latent Factor` alternately.\n",
    "- Because training both factors sometimes very inefficient.\n",
    "- There are two step in ALS.\n",
    "    - Set `Item Latent Factor` as constant for learn to `User Latent Factor`\n",
    "- The original cost function of matrix factorization is below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ min_{q,p} \\sum_{u,i}(r_{ui}-q_i^Tp_u)^2 + \\lambda(||q_i||^2 + ||p_u||^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And there is ALS's cost function.\n",
    "\n",
    "$$ min_{x,y} \\sum c_{ui}(p_{ui}-x_i^Ty_u)^2 + \\lambda(||x_i||^2 + ||y_u||^2) $$\n",
    "\n",
    "- We can see two difference like this\n",
    "    - $ p_{ui} $ is preference score(binary score) in Explicit feedback\n",
    "        - p is 1 if $ r_{ui} > 0 $\n",
    "        - p is 0 if $ r_{ui} = 0 $\n",
    "    - $ c_{ui} $ is confidence for $ p_{ui} $.\n",
    "        - $ c_{ui} = 1 + \\alpha* r_{ui} $\n",
    "        - If there are no score(p score), sparse matrix is very inefficient. (Almost implicit situation has sparse matrix problem)\n",
    "        - c score can solve this problem. If we use this, many zero values turn to non-zero (almost zero) value. So the model's performance is better than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning Step\n",
    "    - 1) Fix Y's latent factor - $ \\frac{dL(x_i)}{dx_i} = -2\\sum c_{ui}(p_{ui}-x_i^Ty_u)*y_u + 2\\lambda x_i $\n",
    "        - Find $ \\frac{dL(x_i)}{dx_i} $ is 0\n",
    "        - $ \\sum_u c_{iu}(x_i^Ty_u)*y_u +\\lambda x_i = \\sum_u c_{iu}(p_{iu})*y_u $\n",
    "        - $ \\sum_u c_{iu}(y_u^Tx_i)*y_u +\\lambda x_i = \\sum_u c_{iu}(p_{iu})*y_u $\n",
    "        - $ (\\sum_u c_{iu}*y_u*y_u^T +\\lambda I )x_i = \\sum_u c_{iu}(p_{iu})*y_u $\n",
    "        - $ x_i = (Y^T C^i Y + \\lambda I)^{-1} Y^T C^i p(i) $\n",
    "    - 2) Fix X's latent factor\n",
    "    - iter loop~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### references\n",
    "- https://yamalab.tistory.com/92\n",
    "- http://yifanhu.net/PUB/cf.pdf\n",
    "- https://yeomko.tistory.com/4\n",
    "- https://datascienceschool.net/view-notebook/fcd3550f11ac4537acec8d18136f2066/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
