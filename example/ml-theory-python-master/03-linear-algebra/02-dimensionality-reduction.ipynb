{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### PCA (Principal Component Analysis)\n",
    "\n",
    "PCA is one of the most popular dimensionality reduction method and feature extraction (not same as feature selection). PCA's main purpose is finding new axis(vector) minimize the variance of data. It is called principal component or principal axis.\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "- If we have matrix X(pxn), the number of x feature is p and the number of row n, we can make the output vector $\\vec{z_p}$ in (kxn) matrix.\n",
    "- k is extracted dimension.\n",
    "- Main equation : $Z = A^TX$\n",
    "    - $Z$ : extracted matrix(==dimension reducted dataset). (p x n) matrix.\n",
    "    - $A^T$ : axis to project data.\n",
    "    - $X$ : original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Find Eigenvector, Eigenvalue\n",
    "\n",
    "- To maximize the variance of projected dataset, we have to project data to `eigenvector`\n",
    "- Why eigenvector? below equations is the reason.\n",
    "    - First, we have to define $f(x)$.\n",
    "        - $ \\underset{\\alpha}{argmax}(Var(\\vec{\\alpha}X)) $\n",
    "        - $ \\underset{\\alpha}{argmax}(\\vec{\\alpha}Var(X)\\vec{\\alpha}) $\n",
    "        - $ \\underset{\\alpha}{argmax}(\\vec{\\alpha}\\Sigma \\vec{\\alpha}) $ = $f(x)$\n",
    "    - And there is constraint.\n",
    "        - $||\\alpha|| = 1$\n",
    "        - $ \\vec{\\alpha}^T \\vec{\\alpha} = 1$\n",
    "        - $ \\vec{\\alpha}^T \\vec{\\alpha} - 1 = g(x)$\n",
    "    - In this situation, we can using Lagrange Multiplier.\n",
    "        - $ L = \\vec{\\alpha}^T \\Sigma \\vec{\\alpha} - \\lambda(\\vec{\\alpha}^T \\vec{\\alpha} - 1) $\n",
    "        - $ \\frac{dL}{d\\vec{\\alpha}} = (\\Sigma-\\lambda)\\vec{\\alpha} = 0 $\n",
    "    - We already know $(A-\\lambda I)V = 0$ by define of eigenvector.\n",
    "    - Therefore, $\\alpha$ is `eigenvector` of $\\Sigma$, and $\\lambda$ is `eigenvalue` of $\\Sigma$.\n",
    "    - Appendix\n",
    "        - $\\Sigma$ is `covariance matrix of A`.\n",
    "        - $\\Sigma = Cov(X) = \\frac{XX^T}{n-1} \\varpropto XX^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Summary\n",
    "\n",
    "- If we have original matrix(dataset) X, $\\frac{AA^T}{n-1} = Cov(A) = \\Sigma$.\n",
    "- We have to find principal component. It means find eigenvector, eigenvalue of $\\Sigma$.\n",
    "- Process\n",
    "    - Find eigenvector, eigenvalue of $\\Sigma$ using below equations.\n",
    "    - $ \\Sigma V = V \\bigwedge $\n",
    "        - $ \\bigwedge = \\begin{bmatrix}\\lambda_1 & 0 & 0\\\\0 & \\lambda_2 & 0\\\\0 & 0 & \\lambda_1 \\end{bmatrix} $\n",
    "        - $ V = [\\vec{v_1}, \\vec{v_2}, \\vec{v_3}] $\n",
    "    - $ \\vec{z_1} = \\vec{v_1}^TX $\n",
    "- Appendix : we can explain the percentage of preservation of the variance of the original data `when extracted feature dimension is 1` by calculate this equation.\n",
    "    - $\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2 + \\lambda_3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Relationship between SVD and PCA\n",
    "\n",
    "- We know PCA's property $ A^TA = V \\bigwedge V^T $.\n",
    "- And SVD's equation is $ A = U\\Sigma V^T $.\n",
    "- Now there are some equations\n",
    "    - $ AA^T = (U\\Sigma V^T)(U\\Sigma V^T)^T $\n",
    "    - $ AA^T = U \\bigwedge U^T = U\\Sigma^2U^T$\n",
    "- Then, we know PCA's $\\bigwedge$ and SVD's $\\Sigma$ is same role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### t-SNE\n",
    "\n",
    "Probabilistic feature reduction by using gaussian distribution and t-distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### references\n",
    "- https://datascienceschool.net/view-notebook/04358acdcf3347fc989c4cfc0ef6121c/\n",
    "- https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/\n",
    "- https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
