{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Armed Bandit\n",
    "----\n",
    "\n",
    "### Concept\n",
    "- MAB is just a mechanism. especially using in S&R(Search and Recommendation) system.\n",
    "- The main concept of MAB is using exploration and exploitation for below situation.\n",
    "- In this chapter, we study about epsilon greedy mab system.\n",
    "\n",
    "### Terms\n",
    "- $ A_t $ : Action. the choice of user in system.\n",
    "- $ R_t $ : Reward. the result of action in system.\n",
    "- $ Q_t(A) $ : Expectation of reward.\n",
    "\n",
    "$$ Q_t(A) = \\frac{sum \\, of \\, rewards \\, when \\, a \\, taken \\, prior \\, to \\, t}{number \\, of \\, times \\, a \\, taken \\, prior \\, to \\, t} $$\n",
    "\n",
    "- Greedy algorithm is always choice the item that what makes the expected reward maximum.\n",
    "\n",
    "----\n",
    "### Model Equation\n",
    "- Simple greedy algorithm doesn't take into account about exploration.So, through the method called epsilon-greedy, we select harmoniously between greedy and random behaviors in probability.\n",
    "- The probability called $ \\epsilon $.\n",
    "- Exploitation of greedy behavior is selected with probability of $ 1- \\epsilon $, and random behavior is selected with probability as much as $ \\epsilon $. In other words, it can be said that the best case is selected as the probability of $ 1- \\epsilon $, and the remaining probability is selected in consideration of diversity.\n",
    "- However, this method also has its drawbacks\n",
    "    - Depending on the value of $ \\epsilon $, there may be insufficient observations among all cases.\n",
    "    - In addition, even if an optimal case is found, the ratio as much as $ \\epsilon $ must be used at random, which can produce unfortunate results from an optimization point of view.\n",
    "- Below code is implementation of e-greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EpsilonGreedy():\n",
    "    def __init__(self, epsilon, counts, values):\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "\n",
    "    def initialize(self, n_arms):\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.array([12, 31, 11, 22])\n",
    "#         self.values = np.zeros(n_arms)\n",
    "    \n",
    "    def select_arm(self):\n",
    "        if random.random() > self.epsilon:\n",
    "            return np.argmax(self.values)\n",
    "        else:\n",
    "            return random.randrange(len(self.values))\n",
    "        \n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = (((n-1) / n) * value) + ((1 / n) * reward)  # 이동 평균으로 업데이트\n",
    "        self.values[chosen_arm] = new_value\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EpsilonGreedy(epsilon=0.5, counts=1, values=1)\n",
    "model.initialize(n_arms=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.select_arm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
